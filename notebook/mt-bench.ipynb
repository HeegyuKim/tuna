{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"../outputs/*/*/prometheus-13b-v1.0/alpaca-eval.json\")\n",
    "dfs = []\n",
    "\n",
    "def filter_score(x):\n",
    "    if x[\"score\"] > 0:\n",
    "        return x[\"score\"]\n",
    "    if \"overall score is\" in x[\"critic\"]:\n",
    "        score = int(x[\"critic\"].split(\"overall score is\")[1].split(\".\")[0].strip().replace(\",\",\"\").split(\" \", 1)[0])\n",
    "        return score\n",
    "    elif \"a score of \" in x[\"critic\"]:\n",
    "        score = x[\"critic\"].split(\"a score of \")[1].split(\" \")[0].split(\".\")[0].split(\",\")[0].strip()\n",
    "        if score:\n",
    "            return int(score)\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "for file in files:\n",
    "    items = []\n",
    "    for item in jsonlines.open(file):\n",
    "        for turn in [\"judge1\", \"judge2\"]:\n",
    "            scores = []\n",
    "            for rubric, judgement in item[turn]:\n",
    "                item[f\"{turn}_{rubric}\"] = filter_score(judgement)\n",
    "        items.append(item)\n",
    "        \n",
    "    df = pd.DataFrame(items)\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df[\"output_len\"] = df.output.apply(len)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
